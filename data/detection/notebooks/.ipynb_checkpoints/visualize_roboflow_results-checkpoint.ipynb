{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# Local YOLO Model Performance Analysis\n",
        "\n",
        "This notebook visualizes and analyzes the results from the local YOLO model inference test.\n",
        "\n",
        "**Prerequisites:** Run the local inference script first:\n",
        "```bash\n",
        "python3 detection/scripts/test_local_model_inference.py --limit 50 --confidence 0.25\n",
        "```\n",
        "\n",
        "This will generate:\n",
        "- `detection/results/inference_results.json`\n",
        "- `detection/results/performance_metrics.json`\n",
        "- `detection/results/sample_images/` (annotated images)\n",
        "\n",
        "## üéØ Analysis Focus\n",
        "- **Success Cases**: Images where container numbers were detected\n",
        "- **Failure Cases**: Images where no detections were made\n",
        "- **Performance Metrics**: Speed, confidence, detection rates\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from pathlib import Path\n",
        "from IPython.display import Image as IPImage, display, HTML\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "from plotly.subplots import make_subplots\n",
        "\n",
        "# Set up plotting style\n",
        "plt.style.use('default')\n",
        "sns.set_palette(\"husl\")\n",
        "%matplotlib inline\n",
        "\n",
        "# Paths\n",
        "RESULTS_DIR = Path(\"../results\")\n",
        "SAMPLE_IMAGES_DIR = RESULTS_DIR / \"sample_images\"\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## üìä Load Results Data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load inference results\n",
        "with open(RESULTS_DIR / \"inference_results.json\", 'r') as f:\n",
        "    results = json.load(f)\n",
        "\n",
        "with open(RESULTS_DIR / \"performance_metrics.json\", 'r') as f:\n",
        "    performance = json.load(f)\n",
        "\n",
        "print(f\"üìä Loaded results for {results['summary']['total_images']} images\")\n",
        "print(f\"üéØ Model: {Path(results['model_info']['model_path']).name}\")\n",
        "print(f\"‚è∞ Test run: {results['model_info']['timestamp']}\")\n",
        "print(f\"üéöÔ∏è Confidence threshold: {results['model_info']['confidence_threshold']}\")\n",
        "print(f\"üè∑Ô∏è Model classes: {results['model_info']['model_classes']}\")\n",
        "\n",
        "# Quick stats\n",
        "detection_rate = performance['detection_rate']\n",
        "avg_confidence = performance['confidence_stats']['mean']\n",
        "avg_time = performance['timing_stats']['avg_inference_time']\n",
        "\n",
        "print(f\"\\nüìà Quick Stats:\")\n",
        "print(f\"   Detection Rate: {detection_rate:.1%}\")\n",
        "print(f\"   Avg Confidence: {avg_confidence:.3f}\")\n",
        "print(f\"   Avg Inference Time: {avg_time:.3f}s ({1/avg_time:.1f} FPS)\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## üîç Success vs Failure Analysis\n",
        "\n",
        "Let's separate the results into successful detections and failures to understand what's working and what isn't.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Separate success and failure cases\n",
        "successful_images = [img for img in results['images'] if img['num_detections'] > 0]\n",
        "failed_images = [img for img in results['images'] if img['num_detections'] == 0]\n",
        "\n",
        "print(f\"‚úÖ Successful detections: {len(successful_images)} ({len(successful_images)/len(results['images']):.1%})\")\n",
        "print(f\"‚ùå Failed detections: {len(failed_images)} ({len(failed_images)/len(results['images']):.1%})\")\n",
        "\n",
        "# Analyze successful cases\n",
        "if successful_images:\n",
        "    success_confidences = []\n",
        "    success_times = []\n",
        "    for img in successful_images:\n",
        "        success_times.append(img['inference_time'])\n",
        "        for det in img['detections']:\n",
        "            success_confidences.append(det['confidence'])\n",
        "    \n",
        "    print(f\"\\nüìä Success Case Analysis:\")\n",
        "    print(f\"   Avg confidence: {np.mean(success_confidences):.3f}\")\n",
        "    print(f\"   Confidence range: {np.min(success_confidences):.3f} - {np.max(success_confidences):.3f}\")\n",
        "    print(f\"   Avg inference time: {np.mean(success_times):.3f}s\")\n",
        "\n",
        "# Analyze failure cases\n",
        "if failed_images:\n",
        "    fail_times = [img['inference_time'] for img in failed_images]\n",
        "    print(f\"\\nüìä Failure Case Analysis:\")\n",
        "    print(f\"   Avg inference time: {np.mean(fail_times):.3f}s\")\n",
        "    print(f\"   Failed filenames (first 10): {[img['filename'] for img in failed_images[:10]]}\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## üñºÔ∏è Visual Inspection: Success Cases\n",
        "\n",
        "Let's look at some successful detections to see what the model is detecting well.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display successful detection examples\n",
        "def display_detection_results(image_data, title, max_images=6):\n",
        "    \"\"\"Display detection results with metadata.\"\"\"\n",
        "    print(f\"\\n{title}\")\n",
        "    print(\"=\" * len(title))\n",
        "    \n",
        "    for i, img in enumerate(image_data[:max_images]):\n",
        "        print(f\"\\nüì∑ {img['filename']} (Expected: {img['expected_code']})\")\n",
        "        \n",
        "        if img['num_detections'] > 0:\n",
        "            for j, det in enumerate(img['detections']):\n",
        "                print(f\"   Detection {j+1}: {det['class']} (confidence: {det['confidence']:.3f})\")\n",
        "                print(f\"   Bounding box: {det['bbox']}\")\n",
        "        else:\n",
        "            print(\"   No detections\")\n",
        "        \n",
        "        print(f\"   Inference time: {img['inference_time']:.3f}s\")\n",
        "        \n",
        "        # Display annotated image if available\n",
        "        annotated_path = SAMPLE_IMAGES_DIR / f\"annotated_{img['filename']}\"\n",
        "        if annotated_path.exists():\n",
        "            print(f\"   üì∏ Annotated image:\")\n",
        "            display(IPImage(str(annotated_path), width=400))\n",
        "        else:\n",
        "            print(f\"   ‚ö†Ô∏è Annotated image not found: {annotated_path}\")\n",
        "\n",
        "# Show successful cases\n",
        "if successful_images:\n",
        "    # Sort by confidence (highest first)\n",
        "    sorted_success = sorted(successful_images, \n",
        "                          key=lambda x: max([d['confidence'] for d in x['detections']]), \n",
        "                          reverse=True)\n",
        "    display_detection_results(sorted_success, \"üéØ TOP SUCCESSFUL DETECTIONS (Highest Confidence)\", max_images=3)\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## ‚ùå Visual Inspection: Failure Cases\n",
        "\n",
        "Now let's examine some failure cases to understand what the model is missing.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Show failure cases\n",
        "if failed_images:\n",
        "    print(f\"\\n‚ùå FAILURE CASES ANALYSIS\")\n",
        "    print(\"=\" * 30)\n",
        "    \n",
        "    # Show some failed images (original images from the dataset)\n",
        "    input_dir = Path(\"../training_data/00_raw/container_code_tbd\")\n",
        "    \n",
        "    for i, img in enumerate(failed_images[:5]):  # Show first 5 failures\n",
        "        print(f\"\\nüì∑ {img['filename']} (Expected: {img['expected_code']})\")\n",
        "        print(f\"   Inference time: {img['inference_time']:.3f}s\")\n",
        "        print(f\"   Status: No detections found\")\n",
        "        \n",
        "        # Try to display the original image\n",
        "        original_path = input_dir / img['filename']\n",
        "        if original_path.exists():\n",
        "            print(f\"   üì∏ Original image:\")\n",
        "            display(IPImage(str(original_path), width=400))\n",
        "        else:\n",
        "            print(f\"   ‚ö†Ô∏è Original image not found: {original_path}\")\n",
        "            \n",
        "        # Also check if there's an annotated version (should be empty)\n",
        "        annotated_path = SAMPLE_IMAGES_DIR / f\"annotated_{img['filename']}\"\n",
        "        if annotated_path.exists():\n",
        "            print(f\"   üì∏ Annotated image (no detections):\")\n",
        "            display(IPImage(str(annotated_path), width=400))\n",
        "else:\n",
        "    print(\"üéâ No failure cases found!\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## üìä Performance Visualizations\n",
        "\n",
        "Let's create some charts to better understand the model's performance patterns.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create performance visualizations\n",
        "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
        "fig.suptitle('Local YOLO Model Performance Analysis', fontsize=16, fontweight='bold')\n",
        "\n",
        "# 1. Detection Success Rate\n",
        "detection_counts = [len(successful_images), len(failed_images)]\n",
        "detection_labels = ['Successful\\nDetections', 'Failed\\nDetections']\n",
        "colors = ['#2ecc71', '#e74c3c']\n",
        "\n",
        "axes[0,0].pie(detection_counts, labels=detection_labels, colors=colors, autopct='%1.1f%%', startangle=90)\n",
        "axes[0,0].set_title('Detection Success Rate')\n",
        "\n",
        "# 2. Confidence Distribution (for successful detections only)\n",
        "if successful_images:\n",
        "    all_confidences = []\n",
        "    for img in successful_images:\n",
        "        for det in img['detections']:\n",
        "            all_confidences.append(det['confidence'])\n",
        "    \n",
        "    axes[0,1].hist(all_confidences, bins=15, color='#3498db', alpha=0.7, edgecolor='black')\n",
        "    axes[0,1].axvline(np.mean(all_confidences), color='red', linestyle='--', \n",
        "                     label=f'Mean: {np.mean(all_confidences):.3f}')\n",
        "    axes[0,1].set_xlabel('Confidence Score')\n",
        "    axes[0,1].set_ylabel('Frequency')\n",
        "    axes[0,1].set_title('Confidence Score Distribution')\n",
        "    axes[0,1].legend()\n",
        "    axes[0,1].grid(True, alpha=0.3)\n",
        "\n",
        "# 3. Inference Time Distribution\n",
        "all_times = [img['inference_time'] for img in results['images']]\n",
        "success_times = [img['inference_time'] for img in successful_images] if successful_images else []\n",
        "fail_times = [img['inference_time'] for img in failed_images] if failed_images else []\n",
        "\n",
        "axes[1,0].hist(success_times, bins=10, alpha=0.7, label='Successful', color='#2ecc71', edgecolor='black')\n",
        "axes[1,0].hist(fail_times, bins=10, alpha=0.7, label='Failed', color='#e74c3c', edgecolor='black')\n",
        "axes[1,0].axvline(np.mean(all_times), color='blue', linestyle='--', \n",
        "                 label=f'Overall Mean: {np.mean(all_times):.3f}s')\n",
        "axes[1,0].set_xlabel('Inference Time (seconds)')\n",
        "axes[1,0].set_ylabel('Frequency')\n",
        "axes[1,0].set_title('Inference Time Distribution')\n",
        "axes[1,0].legend()\n",
        "axes[1,0].grid(True, alpha=0.3)\n",
        "\n",
        "# 4. Performance Summary Table\n",
        "summary_data = [\n",
        "    ['Total Images', results['summary']['total_images']],\n",
        "    ['Successful Detections', len(successful_images)],\n",
        "    ['Detection Rate', f\"{performance['detection_rate']:.1%}\"],\n",
        "    ['Avg Confidence', f\"{performance['confidence_stats']['mean']:.3f}\"],\n",
        "    ['Avg Inference Time', f\"{performance['timing_stats']['avg_inference_time']:.3f}s\"],\n",
        "    ['Inference Speed', f\"{1/performance['timing_stats']['avg_inference_time']:.1f} FPS\"],\n",
        "]\n",
        "\n",
        "axes[1,1].axis('tight')\n",
        "axes[1,1].axis('off')\n",
        "table = axes[1,1].table(cellText=summary_data, \n",
        "                       colLabels=['Metric', 'Value'],\n",
        "                       cellLoc='left',\n",
        "                       loc='center',\n",
        "                       colWidths=[0.6, 0.4])\n",
        "table.auto_set_font_size(False)\n",
        "table.set_fontsize(10)\n",
        "table.scale(1, 2)\n",
        "axes[1,1].set_title('Performance Summary')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## üîç Detailed Analysis & Insights\n",
        "\n",
        "Let's dig deeper into patterns and potential improvements.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Analyze patterns in successful vs failed cases\n",
        "print(\"üîç PATTERN ANALYSIS\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Container code prefix analysis\n",
        "def analyze_container_prefixes(image_list, category_name):\n",
        "    \"\"\"Analyze container code prefixes to find patterns.\"\"\"\n",
        "    prefixes = {}\n",
        "    for img in image_list:\n",
        "        code = img['expected_code']\n",
        "        if len(code) >= 4:\n",
        "            prefix = code[:4]  # First 4 characters (owner code)\n",
        "            prefixes[prefix] = prefixes.get(prefix, 0) + 1\n",
        "    \n",
        "    print(f\"\\nüìä {category_name} Container Owner Codes:\")\n",
        "    sorted_prefixes = sorted(prefixes.items(), key=lambda x: x[1], reverse=True)\n",
        "    for prefix, count in sorted_prefixes[:10]:  # Top 10\n",
        "        print(f\"   {prefix}: {count} images\")\n",
        "    \n",
        "    return prefixes\n",
        "\n",
        "if successful_images:\n",
        "    success_prefixes = analyze_container_prefixes(successful_images, \"SUCCESSFUL\")\n",
        "\n",
        "if failed_images:\n",
        "    fail_prefixes = analyze_container_prefixes(failed_images, \"FAILED\")\n",
        "\n",
        "# Find owner codes that appear in both success and failure\n",
        "if successful_images and failed_images:\n",
        "    common_prefixes = set(success_prefixes.keys()) & set(fail_prefixes.keys())\n",
        "    if common_prefixes:\n",
        "        print(f\"\\n‚öñÔ∏è Owner codes appearing in both success and failure:\")\n",
        "        for prefix in common_prefixes:\n",
        "            success_count = success_prefixes.get(prefix, 0)\n",
        "            fail_count = fail_prefixes.get(prefix, 0)\n",
        "            total = success_count + fail_count\n",
        "            success_rate = success_count / total if total > 0 else 0\n",
        "            print(f\"   {prefix}: {success_count}‚úÖ/{fail_count}‚ùå (success rate: {success_rate:.1%})\")\n",
        "\n",
        "# Performance insights\n",
        "print(f\"\\nüí° KEY INSIGHTS:\")\n",
        "print(f\"   ‚Ä¢ Detection rate: {performance['detection_rate']:.1%} - {'Good' if performance['detection_rate'] > 0.6 else 'Needs improvement'}\")\n",
        "print(f\"   ‚Ä¢ Confidence when detecting: {performance['confidence_stats']['mean']:.3f} - {'High confidence' if performance['confidence_stats']['mean'] > 0.8 else 'Moderate confidence'}\")\n",
        "print(f\"   ‚Ä¢ Speed: {1/performance['timing_stats']['avg_inference_time']:.1f} FPS - {'Real-time capable' if 1/performance['timing_stats']['avg_inference_time'] > 10 else 'May need optimization'}\")\n",
        "\n",
        "# Recommendations\n",
        "print(f\"\\nüéØ RECOMMENDATIONS:\")\n",
        "if performance['detection_rate'] < 0.7:\n",
        "    print(\"   ‚Ä¢ Consider fine-tuning the model on more Cargosnap container images\")\n",
        "    print(\"   ‚Ä¢ Try different confidence thresholds (current: {:.2f})\".format(results['model_info']['confidence_threshold']))\n",
        "\n",
        "if failed_images:\n",
        "    print(\"   ‚Ä¢ Analyze failure cases for common patterns (lighting, angle, image quality)\")\n",
        "    print(\"   ‚Ä¢ Consider data augmentation for underrepresented container types\")\n",
        "\n",
        "print(\"   ‚Ä¢ Model is ready for mobile integration testing\")\n",
        "print(\"   ‚Ä¢ Consider A/B testing with different confidence thresholds in production\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
