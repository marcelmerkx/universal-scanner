{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# Local YOLO Model Performance Analysis\n",
        "\n",
        "This notebook visualizes and analyzes the results from the local YOLO model inference test.\n",
        "\n",
        "**Prerequisites:** Run the local inference script first:\n",
        "```bash\n",
        "python3 detection/scripts/test_local_model_inference.py --limit 50 --confidence 0.25\n",
        "```\n",
        "\n",
        "This will generate:\n",
        "- `detection/results/inference_results.json`\n",
        "- `detection/results/performance_metrics.json`\n",
        "- `detection/results/sample_images/` (annotated images)\n",
        "\n",
        "## ðŸŽ¯ Analysis Focus\n",
        "- **Success Cases**: Images where container numbers were detected\n",
        "- **Failure Cases**: Images where no detections were made\n",
        "- **Performance Metrics**: Speed, confidence, detection rates\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from pathlib import Path\n",
        "from IPython.display import Image as IPImage, display, HTML\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "from plotly.subplots import make_subplots\n",
        "\n",
        "# Set up plotting style\n",
        "plt.style.use('default')\n",
        "sns.set_palette(\"husl\")\n",
        "%matplotlib inline\n",
        "\n",
        "# Paths\n",
        "RESULTS_DIR = Path(\"../results\")\n",
        "SAMPLE_IMAGES_DIR = RESULTS_DIR / \"sample_images\"\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## ðŸ“Š Load Results Data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load inference results\n",
        "with open(RESULTS_DIR / \"inference_results.json\", 'r') as f:\n",
        "    results = json.load(f)\n",
        "\n",
        "with open(RESULTS_DIR / \"performance_metrics.json\", 'r') as f:\n",
        "    performance = json.load(f)\n",
        "\n",
        "print(f\"ðŸ“Š Loaded results for {results['summary']['total_images']} images\")\n",
        "print(f\"ðŸŽ¯ Model: {Path(results['model_info']['model_path']).name}\")\n",
        "print(f\"â° Test run: {results['model_info']['timestamp']}\")\n",
        "print(f\"ðŸŽšï¸ Confidence threshold: {results['model_info']['confidence_threshold']}\")\n",
        "print(f\"ðŸ·ï¸ Model classes: {results['model_info']['model_classes']}\")\n",
        "\n",
        "# Quick stats\n",
        "detection_rate = performance['detection_rate']\n",
        "avg_confidence = performance['confidence_stats']['mean']\n",
        "avg_time = performance['timing_stats']['avg_inference_time']\n",
        "\n",
        "print(f\"\\nðŸ“ˆ Quick Stats:\")\n",
        "print(f\"   Detection Rate: {detection_rate:.1%}\")\n",
        "print(f\"   Avg Confidence: {avg_confidence:.3f}\")\n",
        "print(f\"   Avg Inference Time: {avg_time:.3f}s ({1/avg_time:.1f} FPS)\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## ðŸ” Success vs Failure Analysis\n",
        "\n",
        "Let's separate the results into successful detections and failures to understand what's working and what isn't.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Separate success and failure cases\n",
        "successful_images = [img for img in results['images'] if img['num_detections'] > 0]\n",
        "failed_images = [img for img in results['images'] if img['num_detections'] == 0]\n",
        "\n",
        "print(f\"âœ… Successful detections: {len(successful_images)} ({len(successful_images)/len(results['images']):.1%})\")\n",
        "print(f\"âŒ Failed detections: {len(failed_images)} ({len(failed_images)/len(results['images']):.1%})\")\n",
        "\n",
        "# Analyze successful cases\n",
        "if successful_images:\n",
        "    success_confidences = []\n",
        "    success_times = []\n",
        "    for img in successful_images:\n",
        "        success_times.append(img['inference_time'])\n",
        "        for det in img['detections']:\n",
        "            success_confidences.append(det['confidence'])\n",
        "    \n",
        "    print(f\"\\nðŸ“Š Success Case Analysis:\")\n",
        "    print(f\"   Avg confidence: {np.mean(success_confidences):.3f}\")\n",
        "    print(f\"   Confidence range: {np.min(success_confidences):.3f} - {np.max(success_confidences):.3f}\")\n",
        "    print(f\"   Avg inference time: {np.mean(success_times):.3f}s\")\n",
        "\n",
        "# Analyze failure cases\n",
        "if failed_images:\n",
        "    fail_times = [img['inference_time'] for img in failed_images]\n",
        "    print(f\"\\nðŸ“Š Failure Case Analysis:\")\n",
        "    print(f\"   Avg inference time: {np.mean(fail_times):.3f}s\")\n",
        "    print(f\"   Failed filenames (first 10): {[img['filename'] for img in failed_images[:10]]}\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## ðŸ–¼ï¸ Visual Inspection: Success Cases\n",
        "\n",
        "Let's look at some successful detections to see what the model is detecting well.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display successful detection examples\n",
        "def display_detection_results(image_data, title, max_images=6):\n",
        "    \"\"\"Display detection results with metadata.\"\"\"\n",
        "    print(f\"\\n{title}\")\n",
        "    print(\"=\" * len(title))\n",
        "    \n",
        "    for i, img in enumerate(image_data[:max_images]):\n",
        "        print(f\"\\nðŸ“· {img['filename']} (Expected: {img['expected_code']})\")\n",
        "        \n",
        "        if img['num_detections'] > 0:\n",
        "            for j, det in enumerate(img['detections']):\n",
        "                print(f\"   Detection {j+1}: {det['class']} (confidence: {det['confidence']:.3f})\")\n",
        "                print(f\"   Bounding box: {det['bbox']}\")\n",
        "        else:\n",
        "            print(\"   No detections\")\n",
        "        \n",
        "        print(f\"   Inference time: {img['inference_time']:.3f}s\")\n",
        "        \n",
        "        # Display annotated image if available\n",
        "        annotated_path = SAMPLE_IMAGES_DIR / f\"annotated_{img['filename']}\"\n",
        "        if annotated_path.exists():\n",
        "            print(f\"   ðŸ“¸ Annotated image:\")\n",
        "            display(IPImage(str(annotated_path), width=400))\n",
        "        else:\n",
        "            print(f\"   âš ï¸ Annotated image not found: {annotated_path}\")\n",
        "\n",
        "# Show successful cases\n",
        "if successful_images:\n",
        "    # Sort by confidence (highest first)\n",
        "    sorted_success = sorted(successful_images, \n",
        "                          key=lambda x: max([d['confidence'] for d in x['detections']]), \n",
        "                          reverse=True)\n",
        "    display_detection_results(sorted_success, \"ðŸŽ¯ TOP SUCCESSFUL DETECTIONS (Highest Confidence)\", max_images=3)\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## âŒ Visual Inspection: Failure Cases\n",
        "\n",
        "Now let's examine some failure cases to understand what the model is missing.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Show failure cases\n",
        "if failed_images:\n",
        "    print(f\"\\nâŒ FAILURE CASES ANALYSIS\")\n",
        "    print(\"=\" * 30)\n",
        "    \n",
        "    # Show some failed images (original images from the dataset)\n",
        "    input_dir = Path(\"../training_data/00_raw/container_code_tbd\")\n",
        "    \n",
        "    for i, img in enumerate(failed_images[:5]):  # Show first 5 failures\n",
        "        print(f\"\\nðŸ“· {img['filename']} (Expected: {img['expected_code']})\")\n",
        "        print(f\"   Inference time: {img['inference_time']:.3f}s\")\n",
        "        print(f\"   Status: No detections found\")\n",
        "        \n",
        "        # Try to display the original image\n",
        "        original_path = input_dir / img['filename']\n",
        "        if original_path.exists():\n",
        "            print(f\"   ðŸ“¸ Original image:\")\n",
        "            display(IPImage(str(original_path), width=400))\n",
        "        else:\n",
        "            print(f\"   âš ï¸ Original image not found: {original_path}\")\n",
        "            \n",
        "        # Also check if there's an annotated version (should be empty)\n",
        "        annotated_path = SAMPLE_IMAGES_DIR / f\"annotated_{img['filename']}\"\n",
        "        if annotated_path.exists():\n",
        "            print(f\"   ðŸ“¸ Annotated image (no detections):\")\n",
        "            display(IPImage(str(annotated_path), width=400))\n",
        "else:\n",
        "    print(\"ðŸŽ‰ No failure cases found!\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## ðŸ“Š Performance Visualizations\n",
        "\n",
        "Let's create some charts to better understand the model's performance patterns.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create performance visualizations\n",
        "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
        "fig.suptitle('Local YOLO Model Performance Analysis', fontsize=16, fontweight='bold')\n",
        "\n",
        "# 1. Detection Success Rate\n",
        "detection_counts = [len(successful_images), len(failed_images)]\n",
        "detection_labels = ['Successful\\nDetections', 'Failed\\nDetections']\n",
        "colors = ['#2ecc71', '#e74c3c']\n",
        "\n",
        "axes[0,0].pie(detection_counts, labels=detection_labels, colors=colors, autopct='%1.1f%%', startangle=90)\n",
        "axes[0,0].set_title('Detection Success Rate')\n",
        "\n",
        "# 2. Confidence Distribution (for successful detections only)\n",
        "if successful_images:\n",
        "    all_confidences = []\n",
        "    for img in successful_images:\n",
        "        for det in img['detections']:\n",
        "            all_confidences.append(det['confidence'])\n",
        "    \n",
        "    axes[0,1].hist(all_confidences, bins=15, color='#3498db', alpha=0.7, edgecolor='black')\n",
        "    axes[0,1].axvline(np.mean(all_confidences), color='red', linestyle='--', \n",
        "                     label=f'Mean: {np.mean(all_confidences):.3f}')\n",
        "    axes[0,1].set_xlabel('Confidence Score')\n",
        "    axes[0,1].set_ylabel('Frequency')\n",
        "    axes[0,1].set_title('Confidence Score Distribution')\n",
        "    axes[0,1].legend()\n",
        "    axes[0,1].grid(True, alpha=0.3)\n",
        "\n",
        "# 3. Inference Time Distribution\n",
        "all_times = [img['inference_time'] for img in results['images']]\n",
        "success_times = [img['inference_time'] for img in successful_images] if successful_images else []\n",
        "fail_times = [img['inference_time'] for img in failed_images] if failed_images else []\n",
        "\n",
        "axes[1,0].hist(success_times, bins=10, alpha=0.7, label='Successful', color='#2ecc71', edgecolor='black')\n",
        "axes[1,0].hist(fail_times, bins=10, alpha=0.7, label='Failed', color='#e74c3c', edgecolor='black')\n",
        "axes[1,0].axvline(np.mean(all_times), color='blue', linestyle='--', \n",
        "                 label=f'Overall Mean: {np.mean(all_times):.3f}s')\n",
        "axes[1,0].set_xlabel('Inference Time (seconds)')\n",
        "axes[1,0].set_ylabel('Frequency')\n",
        "axes[1,0].set_title('Inference Time Distribution')\n",
        "axes[1,0].legend()\n",
        "axes[1,0].grid(True, alpha=0.3)\n",
        "\n",
        "# 4. Performance Summary Table\n",
        "summary_data = [\n",
        "    ['Total Images', results['summary']['total_images']],\n",
        "    ['Successful Detections', len(successful_images)],\n",
        "    ['Detection Rate', f\"{performance['detection_rate']:.1%}\"],\n",
        "    ['Avg Confidence', f\"{performance['confidence_stats']['mean']:.3f}\"],\n",
        "    ['Avg Inference Time', f\"{performance['timing_stats']['avg_inference_time']:.3f}s\"],\n",
        "    ['Inference Speed', f\"{1/performance['timing_stats']['avg_inference_time']:.1f} FPS\"],\n",
        "]\n",
        "\n",
        "axes[1,1].axis('tight')\n",
        "axes[1,1].axis('off')\n",
        "table = axes[1,1].table(cellText=summary_data, \n",
        "                       colLabels=['Metric', 'Value'],\n",
        "                       cellLoc='left',\n",
        "                       loc='center',\n",
        "                       colWidths=[0.6, 0.4])\n",
        "table.auto_set_font_size(False)\n",
        "table.set_fontsize(10)\n",
        "table.scale(1, 2)\n",
        "axes[1,1].set_title('Performance Summary')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## ðŸ” Detailed Analysis & Insights\n",
        "\n",
        "Let's dig deeper into patterns and potential improvements.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Analyze patterns in successful vs failed cases\n",
        "print(\"ðŸ” PATTERN ANALYSIS\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Container code prefix analysis\n",
        "def analyze_container_prefixes(image_list, category_name):\n",
        "    \"\"\"Analyze container code prefixes to find patterns.\"\"\"\n",
        "    prefixes = {}\n",
        "    for img in image_list:\n",
        "        code = img['expected_code']\n",
        "        if len(code) >= 4:\n",
        "            prefix = code[:4]  # First 4 characters (owner code)\n",
        "            prefixes[prefix] = prefixes.get(prefix, 0) + 1\n",
        "    \n",
        "    print(f\"\\nðŸ“Š {category_name} Container Owner Codes:\")\n",
        "    sorted_prefixes = sorted(prefixes.items(), key=lambda x: x[1], reverse=True)\n",
        "    for prefix, count in sorted_prefixes[:10]:  # Top 10\n",
        "        print(f\"   {prefix}: {count} images\")\n",
        "    \n",
        "    return prefixes\n",
        "\n",
        "if successful_images:\n",
        "    success_prefixes = analyze_container_prefixes(successful_images, \"SUCCESSFUL\")\n",
        "\n",
        "if failed_images:\n",
        "    fail_prefixes = analyze_container_prefixes(failed_images, \"FAILED\")\n",
        "\n",
        "# Find owner codes that appear in both success and failure\n",
        "if successful_images and failed_images:\n",
        "    common_prefixes = set(success_prefixes.keys()) & set(fail_prefixes.keys())\n",
        "    if common_prefixes:\n",
        "        print(f\"\\nâš–ï¸ Owner codes appearing in both success and failure:\")\n",
        "        for prefix in common_prefixes:\n",
        "            success_count = success_prefixes.get(prefix, 0)\n",
        "            fail_count = fail_prefixes.get(prefix, 0)\n",
        "            total = success_count + fail_count\n",
        "            success_rate = success_count / total if total > 0 else 0\n",
        "            print(f\"   {prefix}: {success_count}âœ…/{fail_count}âŒ (success rate: {success_rate:.1%})\")\n",
        "\n",
        "# Performance insights\n",
        "print(f\"\\nðŸ’¡ KEY INSIGHTS:\")\n",
        "print(f\"   â€¢ Detection rate: {performance['detection_rate']:.1%} - {'Good' if performance['detection_rate'] > 0.6 else 'Needs improvement'}\")\n",
        "print(f\"   â€¢ Confidence when detecting: {performance['confidence_stats']['mean']:.3f} - {'High confidence' if performance['confidence_stats']['mean'] > 0.8 else 'Moderate confidence'}\")\n",
        "print(f\"   â€¢ Speed: {1/performance['timing_stats']['avg_inference_time']:.1f} FPS - {'Real-time capable' if 1/performance['timing_stats']['avg_inference_time'] > 10 else 'May need optimization'}\")\n",
        "\n",
        "# Recommendations\n",
        "print(f\"\\nðŸŽ¯ RECOMMENDATIONS:\")\n",
        "if performance['detection_rate'] < 0.7:\n",
        "    print(\"   â€¢ Consider fine-tuning the model on more Cargosnap container images\")\n",
        "    print(\"   â€¢ Try different confidence thresholds (current: {:.2f})\".format(results['model_info']['confidence_threshold']))\n",
        "\n",
        "if failed_images:\n",
        "    print(\"   â€¢ Analyze failure cases for common patterns (lighting, angle, image quality)\")\n",
        "    print(\"   â€¢ Consider data augmentation for underrepresented container types\")\n",
        "\n",
        "print(\"   â€¢ Model is ready for mobile integration testing\")\n",
        "print(\"   â€¢ Consider A/B testing with different confidence thresholds in production\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
