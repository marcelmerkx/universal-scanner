{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PT Model Testing\n",
    "\n",
    "Test a specified model with a specific folder with images to verify detection accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Imports and setup\n",
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from ultralytics import YOLO\n",
    "from IPython.display import display\n",
    "import ipywidgets as widgets\n",
    "import numpy as np\n",
    "\n",
    "# Set paths and parameters\n",
    "SOURCE_FOLDER = '../containerdoors/images/'\n",
    "SOURCE_FOLDER = os.path.abspath(SOURCE_FOLDER)\n",
    "MODEL_PATH = '../models/unified-detection-75epocc-13072025.pt'   \n",
    "CONFIDENCE_THRESHOLD = 0.3\n",
    "\n",
    "# Load model\n",
    "model = YOLO(MODEL_PATH)\n",
    "model.conf = CONFIDENCE_THRESHOLD\n",
    "\n",
    "# Get list of JPG files\n",
    "image_files = sorted([f for f in os.listdir(SOURCE_FOLDER) if f.lower().endswith('.jpg')])\n",
    "current_index = 0  # For tracking which image is shown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Preprocessing function\n",
    "def preprocess_image(filepath):\n",
    "    img_gray = cv2.imread(filepath, cv2.IMREAD_GRAYSCALE)\n",
    "    if img_gray is None:\n",
    "        raise ValueError(f\"Image not found or can't be read: {filepath}\")\n",
    "    \n",
    "    h_target = 320\n",
    "    h, w = img_gray.shape\n",
    "    scale = h_target / h\n",
    "    new_w = int(w * scale)\n",
    "\n",
    "    img_resized = cv2.resize(img_gray, (new_w, h_target))\n",
    "    pad_w = 320 - new_w\n",
    "    if pad_w < 0:\n",
    "        raise ValueError(f\"Image width after resize exceeds 320px: {filepath}\")\n",
    "\n",
    "    img_padded = cv2.copyMakeBorder(img_resized, 0, 0, 0, pad_w, cv2.BORDER_CONSTANT, value=255)\n",
    "\n",
    "    img_rgb = cv2.cvtColor(img_padded, cv2.COLOR_GRAY2RGB)  # for display\n",
    "    img_tensor = torch.from_numpy(img_rgb).permute(2, 0, 1).float().unsqueeze(0) / 255.0  # for YOLO\n",
    "\n",
    "    return img_tensor, img_rgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Display and inference function\n",
    "def show_image_with_detections(index):\n",
    "    file_path = os.path.join(SOURCE_FOLDER, image_files[index])\n",
    "    img_tensor, img_np = preprocess_image(file_path)  # Now gets both\n",
    "\n",
    "    results = model(img_tensor)[0]\n",
    "    boxes = results.boxes\n",
    "    classes = results.names\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(6, 6))\n",
    "    ax.imshow(img_np)  # Just use the numpy RGB image, no cvtColor needed\n",
    "\n",
    "    if boxes:\n",
    "        for box in boxes:\n",
    "            cls = int(box.cls[0])\n",
    "            conf = float(box.conf[0])\n",
    "            x1, y1, x2, y2 = map(int, box.xyxy[0])\n",
    "            ax.add_patch(plt.Rectangle((x1, y1), x2 - x1, y2 - y1,\n",
    "                                       edgecolor='red', facecolor='none', linewidth=2))\n",
    "            ax.text(x1, y1 - 5, f\"{classes[cls]} {conf:.2f}\", color='white',\n",
    "                    backgroundcolor='red', fontsize=8)\n",
    "\n",
    "    ax.set_title(f\"{index + 1} / {len(image_files)}\")\n",
    "    ax.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1c6041d4ef7422bbcb7b95d267e923f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Button(description='⬅️ Back', style=ButtonStyle()), Button(description='Next ➡️', style=ButtonS…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83a12e9e1bce40f7a8878a9343419543",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Cell 5: UI to navigate images\n",
    "back_button = widgets.Button(description=\"⬅️ Back\")\n",
    "next_button = widgets.Button(description=\"Next ➡️\")\n",
    "out = widgets.Output()\n",
    "\n",
    "def update_ui(change=None):\n",
    "    out.clear_output(wait=True)\n",
    "    with out:\n",
    "        show_image_with_detections(current_index)\n",
    "\n",
    "def on_next_clicked(b):\n",
    "    global current_index\n",
    "    current_index = (current_index + 1) % len(image_files)\n",
    "    update_ui()\n",
    "\n",
    "def on_back_clicked(b):\n",
    "    global current_index\n",
    "    current_index = (current_index - 1) % len(image_files)\n",
    "    update_ui()\n",
    "\n",
    "back_button.on_click(on_back_clicked)\n",
    "next_button.on_click(on_next_clicked)\n",
    "\n",
    "display(widgets.HBox([back_button, next_button]), out)\n",
    "update_ui()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
