# ONNX Output Format Discovery

**Date**: 2025-06-27  
**Critical Discovery**: ONNX Runtime React Native returns nested arrays, NOT flat Float32Array

## The Problem

We experienced extremely low confidence scores (0.01-0.05) and garbage bounding box data when processing ONNX model outputs. The issue was caused by incorrect data access patterns.

## Root Cause

ONNX Runtime for React Native returns outputs in a **nested array format**, not the expected flat `Float32Array` that desktop/server versions use.

## Wrong Approach (Causes Garbage Data)

```typescript
// ‚ùå WRONG - This treats nested arrays as flat data
const outputData = output.data as Float32Array;
const centerX = outputData[i];
const centerY = outputData[i + 1]; 
const width = outputData[i + 2];
const height = outputData[i + 3];
```

## Correct Approach

```typescript
// ‚úÖ CORRECT - Proper nested array handling
const raw3d = output.value as number[][][];
const preds2d = raw3d[0]; // Extract 2D array from 3D wrapper

// Handle both possible orientations: [ATTRIBUTES, N] or [N, ATTRIBUTES]
const attributes = 9; // 4 bbox + 5 classes for YOLOv8n model
const predsAlongLastDim = preds2d[0].length !== attributes;

function getVal(anchorIdx: number, featureIdx: number): number {
  return predsAlongLastDim 
    ? preds2d[featureIdx][anchorIdx]  // Format: [ATTRIBUTES, N]
    : preds2d[anchorIdx][featureIdx]; // Format: [N, ATTRIBUTES]
}

// Now access data correctly
const centerX = getVal(i, 0);
const centerY = getVal(i, 1);
const width = getVal(i, 2);
const height = getVal(i, 3);
const confidence = getVal(i, 4);
```

## Model Output Shape Analysis

For our YOLOv8n v7 container detection model:
- **Expected Shape**: `[1, 9, 8400]` or `[1, 8400, 9]`
- **Attributes**: 9 (4 bbox + 5 classes)
- **Anchors**: 8400 detection points

The key insight is that the first dimension (batch size = 1) is preserved, but the data structure is a **3D nested array**, not a flat buffer.

## Implementation Reference

See working implementation in:
`/ContainerCameraApp/android/app/src/main/java/com/cargosnap/app/YoloBridgeModule.kt`

## Impact

This discovery fixed:
- ‚ùå Confidence scores jumping from 0.01 ‚Üí ‚úÖ 0.84+
- ‚ùå Random bounding boxes ‚Üí ‚úÖ Accurate object detection
- ‚ùå Model appearing "broken" ‚Üí ‚úÖ Proper YOLO inference

## Key Takeaway

**ONNX Runtime React Native != ONNX Runtime Desktop/Server**

Always check the actual data structure returned by `output.value` rather than assuming it matches documentation for other platforms.


# ONNX Output Format Discovery - Critical Finding

**Date**: 2025-06-27  
**Context**: Mobile app v7 model integration debugging  
**Issue**: Mobile app getting 6% max confidence vs Python 97% accuracy  

## üö® Root Cause Discovered

The fundamental issue was a **complete misunderstanding of ONNX Runtime React Native output format**.

### ‚ùå What We Were Doing Wrong

```typescript
// INCORRECT - treating output as flat Float32Array
const outputData = output.data as Float32Array;
const centerX = outputData[i];
const centerY = outputData[numPredictions + i];
```

### ‚úÖ What The Working Implementation Does

```kotlin
// CORRECT - output is nested array structure
val raw3d = out[0].value as Array<Array<FloatArray>>
val (code, _) = postprocess(raw3d[0])  // Pass 2D slice
```

## üìä The Real ONNX Output Structure

### For Character Recognition (Legacy)
- **Shape**: `[1, 40, 8400]`
- **Format**: `Array<Array<FloatArray>>`
- **Access**: `output.value[0]` gives `Array<FloatArray>` of shape `[40, 8400]`

### For Object Detection (Current v7 Model)
- **Shape**: `[1, 9, 8400]` 
- **Format**: `Array<Array<FloatArray>>`
- **Access**: `output.value[0]` gives `Array<FloatArray>` of shape `[9, 8400]`
- **Attributes**: 4 bbox coords + 5 class scores = 9 total

## üîÑ Shape Adaptability

The working implementation handles **two possible orientations**:

```kotlin
val predsAlongLastDim = preds[0].size != attributes
fun getVal(anchorIdx: Int, featureIdx: Int): Float {
  return if (predsAlongLastDim) {
    preds[featureIdx][anchorIdx]      // [ATTRIBUTES, N] format
  } else {
    preds[anchorIdx][featureIdx]      // [N, ATTRIBUTES] format  
  }
}
```

This means the model output can be either:
- `[9, 8400]` - attributes first (9 features √ó 8400 anchors)
- `[8400, 9]` - anchors first (8400 anchors √ó 9 features)

## üéØ Key Differences: Character vs Object Detection

| Aspect | Character Recognition | Object Detection |
|--------|----------------------|------------------|
| **Classes** | 36 (0-9, A-Z) | 5 (container_h, container_v, etc.) |
| **Attributes** | 4 + 36 = 40 | 4 + 5 = 9 |
| **Output Shape** | `[1, 40, 8400]` | `[1, 9, 8400]` |
| **Confidence Threshold** | 0.25 | Should be ~0.5 |
| **Post-processing** | Take top 11, sort by Y | NMS + confidence filter |

## üîç Evidence From Logs

Our incorrect implementation was producing:
```
Max confidence found: 0.0610 (6.1%)
Found 8355 raw detections before NMS
After NMS: 1749 detections  
Final detections after confidence filter (0.1): 0
```

This suggests we were reading **garbage data** due to incorrect array indexing, not actual low-confidence predictions.

## üõ†Ô∏è Required Fix

1. **Change output parsing**:
   ```typescript
   // FROM: output.data as Float32Array
   // TO: output.value as number[][][]
   const raw3d = output.value as number[][][];
   const preds2d = raw3d[0]; // Shape: [9, 8400] or [8400, 9]
   ```

2. **Implement shape detection**:
   ```typescript
   const attributes = 9; // 4 bbox + 5 classes for v7 model
   const predsAlongLastDim = preds2d[0].length !== attributes;
   ```

3. **Use adaptive indexing**:
   ```typescript
   function getVal(anchorIdx: number, featureIdx: number): number {
     return predsAlongLastDim 
       ? preds2d[featureIdx][anchorIdx]  // [9, 8400]
       : preds2d[anchorIdx][featureIdx]; // [8400, 9]
   }
   ```

## üìö Reference Implementation

See: `/ContainerCameraApp/android/app/src/main/java/com/cargosnap/app/YoloBridgeModule.kt`
- Lines 90-96: Correct output extraction
- Lines 152-211: Proper postprocessing with shape detection
- Lines 159-165: Adaptive indexing implementation

## üéØ Next Steps

1. ‚úÖ Document this finding (this file)
2. ‚úÖ Update project memory 
3. üîÑ Restore confidence threshold to 0.5 (50%)
4. üîÑ Implement correct ONNX output parsing
5. üîÑ Test with corrected implementation

---

**Critical Lesson**: Always verify the actual data structure returned by external libraries, especially when porting between platforms or frameworks. The difference between `output.data` and `output.value` was the key to unlocking proper model performance.